{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13218942,"sourceType":"datasetVersion","datasetId":8378813}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification,\n    TrainingArguments, \n    Trainer,\n    DataCollatorWithPadding\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport torch\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:51.095658Z","iopub.execute_input":"2025-09-30T15:21:51.096027Z","iopub.status.idle":"2025-09-30T15:21:51.101929Z","shell.execute_reply.started":"2025-09-30T15:21:51.096002Z","shell.execute_reply":"2025-09-30T15:21:51.100834Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"\n# ============================================\n# 1. LOAD AND PREPARE DATA\n# ============================================\ndf = pd.read_json('/kaggle/input/dataset/data.jsonl', lines=True)\nprint(f\"Original dataset: {len(df)} samples\")\nprint(f\"AI: {df['label'].sum()}, Human: {(df['label'] == 0).sum()}\")\n\n# Sample balanced subset (200 samples total - adjust as needed)\nn_samples = 100  # per class\n\nai_samples = df[df['label'] == 1].sample(n=min(n_samples, len(df[df['label'] == 1])), random_state=42)\nhuman_samples = df[df['label'] == 0].sample(n=min(n_samples, len(df[df['label'] == 0])), random_state=42)\n\n# Combine and shuffle\nsmall_df = pd.concat([ai_samples, human_samples]).sample(frac=1, random_state=42).reset_index(drop=True)\nprint(f\"\\nTraining with: {len(small_df)} samples\")\nprint(f\"AI: {small_df['label'].sum()}, Human: {(small_df['label'] == 0).sum()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:51.103454Z","iopub.execute_input":"2025-09-30T15:21:51.103750Z","iopub.status.idle":"2025-09-30T15:21:51.172087Z","shell.execute_reply.started":"2025-09-30T15:21:51.103721Z","shell.execute_reply":"2025-09-30T15:21:51.171072Z"}},"outputs":[{"name":"stdout","text":"Original dataset: 5410 samples\nAI: 2742, Human: 2668\n\nTraining with: 200 samples\nAI: 100, Human: 100\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"\n# ============================================\n# 2. TRAIN-TEST SPLIT\n# ============================================\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    small_df['content'].tolist(), \n    small_df['label'].tolist(),\n    test_size=0.2, \n    random_state=42,\n    stratify=small_df['label']\n)\n\nprint(f\"\\nTrain samples: {len(train_texts)}\")\nprint(f\"Validation samples: {len(val_texts)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:51.173674Z","iopub.execute_input":"2025-09-30T15:21:51.174001Z","iopub.status.idle":"2025-09-30T15:21:51.182978Z","shell.execute_reply.started":"2025-09-30T15:21:51.173979Z","shell.execute_reply":"2025-09-30T15:21:51.181820Z"}},"outputs":[{"name":"stdout","text":"\nTrain samples: 160\nValidation samples: 40\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"\n# ============================================\n# 3. LOAD MODEL AND TOKENIZER\n# ============================================\nmodel_name = \"openai-community/roberta-base-openai-detector\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2,\n    ignore_mismatched_sizes=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:51.184011Z","iopub.execute_input":"2025-09-30T15:21:51.184339Z","iopub.status.idle":"2025-09-30T15:21:51.946764Z","shell.execute_reply.started":"2025-09-30T15:21:51.184318Z","shell.execute_reply":"2025-09-30T15:21:51.946052Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at openai-community/roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"\n# ============================================\n# 4. TOKENIZE DATA\n# ============================================\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['text'], \n        padding=False,  # Will pad dynamically during training\n        truncation=True, \n        max_length=512\n    )\n\n# Create HuggingFace datasets\ntrain_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})\nval_dataset = Dataset.from_dict({'text': val_texts, 'label': val_labels})\n\n# Tokenize\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:51.948955Z","iopub.execute_input":"2025-09-30T15:21:51.949256Z","iopub.status.idle":"2025-09-30T15:21:52.144199Z","shell.execute_reply.started":"2025-09-30T15:21:51.949228Z","shell.execute_reply":"2025-09-30T15:21:52.143268Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/160 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36a776453f2c4a369d9d8e4ddfa687f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"738f570f84124733a50c0503329fc001"}},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"\n# ============================================\n# 5. METRICS\n# ============================================\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n    acc = accuracy_score(labels, preds)\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:52.145199Z","iopub.execute_input":"2025-09-30T15:21:52.145443Z","iopub.status.idle":"2025-09-30T15:21:52.151001Z","shell.execute_reply.started":"2025-09-30T15:21:52.145424Z","shell.execute_reply":"2025-09-30T15:21:52.149993Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"\n# ============================================\n# 6. CUSTOM PROGRESS CALLBACK\n# ============================================\nfrom transformers import TrainerCallback\nimport sys\nimport time\n\nclass ProgressBarCallback(TrainerCallback):\n    def __init__(self, num_epochs, num_steps_per_epoch):\n        self.num_epochs = num_epochs\n        self.num_steps_per_epoch = num_steps_per_epoch\n        self.total_steps = num_epochs * num_steps_per_epoch\n        self.current_step = 0\n        self.current_epoch = 0\n        self.epoch_start_time = None\n        self.training_start_time = None\n        self.best_accuracy = 0.0\n        \n    def on_train_begin(self, args, state, control, **kwargs):\n        self.training_start_time = time.time()\n        print(\"\\n\" + \"=\"*70)\n        print(\"ğŸš€ TRAINING STARTED\")\n        print(\"=\"*70)\n        print(f\"Total Epochs: {self.num_epochs} | Steps per Epoch: {self.num_steps_per_epoch}\")\n        print(f\"Total Steps: {self.total_steps}\")\n        print(\"=\"*70 + \"\\n\")\n        \n    def on_epoch_begin(self, args, state, control, **kwargs):\n        self.current_epoch = int(state.epoch) if state.epoch else 0\n        self.epoch_start_time = time.time()\n        print(f\"\\n{'='*70}\")\n        print(f\"ğŸ“š EPOCH {self.current_epoch + 1}/{self.num_epochs}\")\n        print(f\"{'='*70}\")\n        \n    def on_step_end(self, args, state, control, **kwargs):\n        self.current_step = state.global_step\n        \n        # Calculate progress\n        epoch_progress = (state.global_step % self.num_steps_per_epoch) / self.num_steps_per_epoch\n        total_progress = state.global_step / self.total_steps\n        \n        # Create progress bar\n        bar_length = 40\n        filled_length = int(bar_length * epoch_progress)\n        bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)\n        \n        # Get current loss\n        current_loss = state.log_history[-1].get('loss', 0) if state.log_history else 0\n        \n        # Calculate ETA\n        elapsed = time.time() - self.epoch_start_time\n        steps_done = (state.global_step % self.num_steps_per_epoch) + 1\n        steps_remaining = self.num_steps_per_epoch - steps_done\n        eta = (elapsed / steps_done) * steps_remaining if steps_done > 0 else 0\n        \n        # Print progress\n        sys.stdout.write(f'\\r')\n        sys.stdout.write(f'[{bar}] {epoch_progress*100:.1f}% | Step {steps_done}/{self.num_steps_per_epoch} | Loss: {current_loss:.4f} | ETA: {eta:.0f}s')\n        sys.stdout.flush()\n        \n    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n        if metrics:\n            print(\"\\n\")\n            print(f\"\\n{'â”€'*70}\")\n            print(\"ğŸ“Š VALIDATION RESULTS:\")\n            print(f\"{'â”€'*70}\")\n            \n            # Extract and display metrics\n            eval_loss = metrics.get('eval_loss', 0)\n            eval_acc = metrics.get('eval_accuracy', 0)\n            eval_f1 = metrics.get('eval_f1', 0)\n            eval_precision = metrics.get('eval_precision', 0)\n            eval_recall = metrics.get('eval_recall', 0)\n            \n            # Update best accuracy\n            if eval_acc > self.best_accuracy:\n                self.best_accuracy = eval_acc\n                best_marker = \" ğŸŒŸ NEW BEST!\"\n            else:\n                best_marker = \"\"\n            \n            print(f\"  Accuracy:  {eval_acc*100:.2f}%{best_marker}\")\n            print(f\"  Loss:      {eval_loss:.4f}\")\n            print(f\"  F1 Score:  {eval_f1:.4f}\")\n            print(f\"  Precision: {eval_precision:.4f}\")\n            print(f\"  Recall:    {eval_recall:.4f}\")\n            print(f\"{'â”€'*70}\")\n            \n            # Progress bar for overall training\n            overall_progress = (self.current_epoch + 1) / self.num_epochs\n            bar_length = 50\n            filled = int(bar_length * overall_progress)\n            bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)\n            print(f\"\\nğŸ“ˆ Overall Progress: [{bar}] {overall_progress*100:.1f}%\")\n            print(f\"   Epochs Completed: {self.current_epoch + 1}/{self.num_epochs}\")\n            \n    def on_train_end(self, args, state, control, **kwargs):\n        total_time = time.time() - self.training_start_time\n        minutes = int(total_time // 60)\n        seconds = int(total_time % 60)\n        \n        print(f\"\\n\\n{'='*70}\")\n        print(\"âœ… TRAINING COMPLETE!\")\n        print(f\"{'='*70}\")\n        print(f\"  Total Time: {minutes}m {seconds}s\")\n        print(f\"  Best Validation Accuracy: {self.best_accuracy*100:.2f}%\")\n        print(f\"  Total Steps: {self.total_steps}\")\n        print(f\"{'='*70}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:52.152143Z","iopub.execute_input":"2025-09-30T15:21:52.152411Z","iopub.status.idle":"2025-09-30T15:21:52.170653Z","shell.execute_reply.started":"2025-09-30T15:21:52.152391Z","shell.execute_reply":"2025-09-30T15:21:52.169829Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"\n# ============================================\n# 7. TRAINING CONFIGURATION\n# ============================================\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,  # Start with 3 epochs for small dataset\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=5,  # Log every 5 steps for more frequent updates\n    eval_strategy=\"steps\",  # Changed from \"epoch\" to \"steps\"\n    eval_steps=10,  # Validate every 10 steps instead of every epoch\n    save_strategy=\"steps\",  # Save every 10 steps too\n    save_steps=10,\n    save_total_limit=3,  # Keep only 3 best checkpoints\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    learning_rate=2e-5,\n    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n    report_to=\"none\",  # Disable wandb/tensorboard\n    disable_tqdm=True  # Disable default progress bar\n)\n\n# Data collator for dynamic padding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Calculate steps per epoch\nsteps_per_epoch = len(train_dataset) // training_args.per_device_train_batch_size\nif len(train_dataset) % training_args.per_device_train_batch_size != 0:\n    steps_per_epoch += 1\n\nprint(f\"âš™ï¸  Training Configuration:\")\nprint(f\"   â€¢ Validation every {training_args.eval_steps} steps\")\nprint(f\"   â€¢ Steps per epoch: {steps_per_epoch}\")\nprint(f\"   â€¢ Total validations per epoch: ~{steps_per_epoch // training_args.eval_steps}\")\n\n# Initialize custom progress callback\nprogress_callback = ProgressBarCallback(\n    num_epochs=training_args.num_train_epochs,\n    num_steps_per_epoch=steps_per_epoch\n)\n\n# ============================================\n# 8. TRAIN MODEL\n# ============================================\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[progress_callback]  # Add custom callback\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:52.171632Z","iopub.execute_input":"2025-09-30T15:21:52.171929Z","iopub.status.idle":"2025-09-30T15:21:52.230919Z","shell.execute_reply.started":"2025-09-30T15:21:52.171908Z","shell.execute_reply":"2025-09-30T15:21:52.229983Z"}},"outputs":[{"name":"stdout","text":"âš™ï¸  Training Configuration:\n   â€¢ Validation every 10 steps\n   â€¢ Steps per epoch: 20\n   â€¢ Total validations per epoch: ~2\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1514412300.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:21:52.232538Z","iopub.execute_input":"2025-09-30T15:21:52.232864Z","iopub.status.idle":"2025-09-30T15:36:50.862096Z","shell.execute_reply.started":"2025-09-30T15:21:52.232842Z","shell.execute_reply":"2025-09-30T15:36:50.861278Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nğŸš€ TRAINING STARTED\n======================================================================\nTotal Epochs: 3 | Steps per Epoch: 20\nTotal Steps: 60\n======================================================================\n\n\n======================================================================\nğŸ“š EPOCH 1/3\n======================================================================\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25.0% | Step 6/20 | Loss: 0.0000 | ETA: 155s{'loss': 2.4203, 'grad_norm': 139.97254943847656, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.25}\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 50.0% | Step 11/20 | Loss: 2.4203 | ETA: 115s{'loss': 2.7389, 'grad_norm': 91.70169067382812, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.5}\n{'eval_loss': 3.4577457904815674, 'eval_accuracy': 0.45, 'eval_f1': 0.5217391304347826, 'eval_precision': 0.46153846153846156, 'eval_recall': 0.6, 'eval_runtime': 15.779, 'eval_samples_per_second': 2.535, 'eval_steps_per_second': 0.317, 'epoch': 0.5}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  45.00% ğŸŒŸ NEW BEST!\n  Loss:      3.4577\n  F1 Score:  0.5217\n  Precision: 0.4615\n  Recall:    0.6000\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 33.3%\n   Epochs Completed: 1/3\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 75.0% | Step 16/20 | Loss: 0.0000 | ETA: 56ss{'loss': 1.188, 'grad_norm': 47.14594650268555, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.75}\n[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% | Step 1/20 | Loss: 1.1880 | ETA: 5289s{'loss': 1.8477, 'grad_norm': 124.76620483398438, 'learning_rate': 7.600000000000001e-06, 'epoch': 1.0}\n{'eval_loss': 2.238682508468628, 'eval_accuracy': 0.4, 'eval_f1': 0.5199999999999999, 'eval_precision': 0.43333333333333335, 'eval_recall': 0.65, 'eval_runtime': 15.6071, 'eval_samples_per_second': 2.563, 'eval_steps_per_second': 0.32, 'epoch': 1.0}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  40.00%\n  Loss:      2.2387\n  F1 Score:  0.5200\n  Precision: 0.4333\n  Recall:    0.6500\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 33.3%\n   Epochs Completed: 1/3\n\n======================================================================\nğŸ“š EPOCH 2/3\n======================================================================\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25.0% | Step 6/20 | Loss: 0.0000 | ETA: 159s{'loss': 0.6897, 'grad_norm': 44.473228454589844, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.25}\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 50.0% | Step 11/20 | Loss: 0.6897 | ETA: 112s{'loss': 0.7376, 'grad_norm': 73.11672973632812, 'learning_rate': 1.16e-05, 'epoch': 1.5}\n{'eval_loss': 1.206190824508667, 'eval_accuracy': 0.45, 'eval_f1': 0.5217391304347826, 'eval_precision': 0.46153846153846156, 'eval_recall': 0.6, 'eval_runtime': 15.6722, 'eval_samples_per_second': 2.552, 'eval_steps_per_second': 0.319, 'epoch': 1.5}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  45.00%\n  Loss:      1.2062\n  F1 Score:  0.5217\n  Precision: 0.4615\n  Recall:    0.6000\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 66.7%\n   Epochs Completed: 2/3\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 75.0% | Step 16/20 | Loss: 0.0000 | ETA: 54ss{'loss': 0.5657, 'grad_norm': 16.875686645507812, 'learning_rate': 1.3600000000000002e-05, 'epoch': 1.75}\n[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% | Step 1/20 | Loss: 0.5657 | ETA: 5274s{'loss': 0.5464, 'grad_norm': 20.659337997436523, 'learning_rate': 1.5600000000000003e-05, 'epoch': 2.0}\n{'eval_loss': 0.9279433488845825, 'eval_accuracy': 0.525, 'eval_f1': 0.5777777777777778, 'eval_precision': 0.52, 'eval_recall': 0.65, 'eval_runtime': 15.7545, 'eval_samples_per_second': 2.539, 'eval_steps_per_second': 0.317, 'epoch': 2.0}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  52.50% ğŸŒŸ NEW BEST!\n  Loss:      0.9279\n  F1 Score:  0.5778\n  Precision: 0.5200\n  Recall:    0.6500\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 66.7%\n   Epochs Completed: 2/3\n\n======================================================================\nğŸ“š EPOCH 3/3\n======================================================================\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 25.0% | Step 6/20 | Loss: 0.0000 | ETA: 139s{'loss': 0.3079, 'grad_norm': 8.609981536865234, 'learning_rate': 1.76e-05, 'epoch': 2.25}\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 50.0% | Step 11/20 | Loss: 0.3079 | ETA: 95ss{'loss': 0.1735, 'grad_norm': 13.189409255981445, 'learning_rate': 1.9600000000000002e-05, 'epoch': 2.5}\n{'eval_loss': 0.9482189416885376, 'eval_accuracy': 0.525, 'eval_f1': 0.5581395348837209, 'eval_precision': 0.5217391304347826, 'eval_recall': 0.6, 'eval_runtime': 15.6068, 'eval_samples_per_second': 2.563, 'eval_steps_per_second': 0.32, 'epoch': 2.5}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  52.50%\n  Loss:      0.9482\n  F1 Score:  0.5581\n  Precision: 0.5217\n  Recall:    0.6000\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%\n   Epochs Completed: 3/3\n[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 75.0% | Step 16/20 | Loss: 0.0000 | ETA: 54s{'loss': 0.2912, 'grad_norm': 15.943950653076172, 'learning_rate': 1.2e-05, 'epoch': 2.75}\n[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0.0% | Step 1/20 | Loss: 0.2912 | ETA: 5441s{'loss': 0.1306, 'grad_norm': 13.062898635864258, 'learning_rate': 2.0000000000000003e-06, 'epoch': 3.0}\n{'eval_loss': 1.1757831573486328, 'eval_accuracy': 0.575, 'eval_f1': 0.6530612244897959, 'eval_precision': 0.5517241379310345, 'eval_recall': 0.8, 'eval_runtime': 15.6562, 'eval_samples_per_second': 2.555, 'eval_steps_per_second': 0.319, 'epoch': 3.0}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  57.50% ğŸŒŸ NEW BEST!\n  Loss:      1.1758\n  F1 Score:  0.6531\n  Precision: 0.5517\n  Recall:    0.8000\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%\n   Epochs Completed: 3/3\n{'train_runtime': 898.0815, 'train_samples_per_second': 0.534, 'train_steps_per_second': 0.067, 'train_loss': 0.9697819451491038, 'epoch': 3.0}\n\n\n======================================================================\nâœ… TRAINING COMPLETE!\n======================================================================\n  Total Time: 14m 58s\n  Best Validation Accuracy: 57.50%\n  Total Steps: 60\n======================================================================\n\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=60, training_loss=0.9697819451491038, metrics={'train_runtime': 898.0815, 'train_samples_per_second': 0.534, 'train_steps_per_second': 0.067, 'train_loss': 0.9697819451491038, 'epoch': 3.0})"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"\n# ============================================\n# 8. EVALUATE\n# ============================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*60)\n\neval_results = trainer.evaluate()\nfor key, value in eval_results.items():\n    print(f\"{key}: {value:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:36:50.863507Z","iopub.execute_input":"2025-09-30T15:36:50.864263Z","iopub.status.idle":"2025-09-30T15:37:07.598998Z","shell.execute_reply.started":"2025-09-30T15:36:50.864239Z","shell.execute_reply":"2025-09-30T15:37:07.598024Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEVALUATION RESULTS\n============================================================\n{'eval_loss': 1.1757831573486328, 'eval_accuracy': 0.575, 'eval_f1': 0.6530612244897959, 'eval_precision': 0.5517241379310345, 'eval_recall': 0.8, 'eval_runtime': 16.7231, 'eval_samples_per_second': 2.392, 'eval_steps_per_second': 0.299, 'epoch': 3.0}\n\n\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nğŸ“Š VALIDATION RESULTS:\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Accuracy:  57.50%\n  Loss:      1.1758\n  F1 Score:  0.6531\n  Precision: 0.5517\n  Recall:    0.8000\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“ˆ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%\n   Epochs Completed: 3/3\neval_loss: 1.1758\neval_accuracy: 0.5750\neval_f1: 0.6531\neval_precision: 0.5517\neval_recall: 0.8000\neval_runtime: 16.7231\neval_samples_per_second: 2.3920\neval_steps_per_second: 0.2990\nepoch: 3.0000\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"\n# ============================================\n# 9. SAVE MODEL\n# ============================================\nmodel.save_pretrained('./finetuned_roberta_detector')\ntokenizer.save_pretrained('./finetuned_roberta_detector')\nprint(\"\\nâœ… Model saved to './finetuned_roberta_detector'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:37:07.601213Z","iopub.execute_input":"2025-09-30T15:37:07.601485Z","iopub.status.idle":"2025-09-30T15:37:09.426753Z","shell.execute_reply.started":"2025-09-30T15:37:07.601466Z","shell.execute_reply":"2025-09-30T15:37:09.425737Z"}},"outputs":[{"name":"stdout","text":"\nâœ… Model saved to './finetuned_roberta_detector'\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"\n# ============================================\n# 10. TEST ON NEW SAMPLES\n# ============================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"TESTING ON SAMPLE TEXTS\")\nprint(\"=\"*60)\n\nfrom transformers import pipeline\n\n# Load your fine-tuned model\npipe = pipeline(\"text-classification\", model='./finetuned_roberta_detector', device=0 if torch.cuda.is_available() else -1)\n\n# Test samples\ntest_samples = [\n    # Social Media / Casual Human Text\n    (\"Check out this amazing sunset I captured! #nature #photography\", \"Human\"),\n    (\"Just finished my morning coffee â˜• Ready to tackle the day!\", \"Human\"),\n    (\"Can't believe I forgot my keys again ğŸ¤¦â€â™‚ï¸ third time this week lol\", \"Human\"),\n    (\"Anyone else watching the game tonight? LFG!! ğŸˆ\", \"Human\"),\n    (\"Tried that new restaurant downtown - totally worth the hype! The pasta was *chef's kiss*\", \"Human\"),\n    \n    # Academic / Formal AI Text\n    (\"Statistical inference is the process of using data analysis to infer properties of an underlying distribution.\", \"AI\"),\n    (\"Machine learning algorithms utilize patterns in data to make predictions without being explicitly programmed.\", \"AI\"),\n    (\"The implementation of neural networks involves multiple layers of interconnected nodes that process information.\", \"AI\"),\n    (\"Quantitative analysis demonstrates a significant correlation between the variables under investigation.\", \"AI\"),\n    (\"The methodology employed in this study encompasses both qualitative and quantitative research approaches.\", \"AI\"),\n    \n    # Conversational Human Text\n    (\"Honestly, I have no idea what I'm doing half the time but somehow it works out\", \"Human\"),\n    (\"My dog just stole my sandwich right off the table ğŸ˜‚ can't even be mad\", \"Human\"),\n    (\"Does anyone know a good place to buy cheap textbooks? College is expensive af\", \"Human\"),\n    \n    # Technical AI Text\n    (\"The algorithm processes input data through convolutional layers to extract relevant features for classification.\", \"AI\"),\n    (\"Database normalization reduces redundancy and improves data integrity through systematic decomposition of tables.\", \"AI\"),\n    (\"Cloud computing infrastructure provides scalable resources through virtualization and distributed systems.\", \"AI\"),\n    \n    # Creative Human Writing\n    (\"The rain tapped gently on my window as I curled up with my favorite book and hot chocolate\", \"Human\"),\n    (\"Running late as usual, grabbed my bag and practically flew out the door\", \"Human\"),\n    (\"Sometimes I wonder if my plants judge me for forgetting to water them ğŸŒ±\", \"Human\"),\n    \n    # Formal AI Writing\n    (\"Contemporary research indicates that climate change significantly impacts global biodiversity patterns.\", \"AI\"),\n    (\"Economic indicators suggest substantial fluctuations in market volatility during the observed period.\", \"AI\"),\n    (\"The experimental results demonstrate statistically significant differences between the control and treatment groups.\", \"AI\"),\n]\n\nfor text, expected in test_samples:\n    result = pipe(text)\n    predicted = \"AI\" if result[0]['label'] == 'LABEL_1' else \"Human\"\n    confidence = result[0]['score']\n    \n    status = \"âœ…\" if predicted == expected else \"âŒ\"\n    print(f\"\\n{status} Expected: {expected} | Predicted: {predicted} ({confidence:.1%})\")\n    print(f\"Text: {text[:100]}...\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T15:38:19.387988Z","iopub.execute_input":"2025-09-30T15:38:19.388363Z","iopub.status.idle":"2025-09-30T15:38:21.345941Z","shell.execute_reply.started":"2025-09-30T15:38:19.388337Z","shell.execute_reply":"2025-09-30T15:38:21.345136Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTESTING ON SAMPLE TEXTS\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Expected: Human | Predicted: Human (51.5%)\nText: Check out this amazing sunset I captured! #nature #photography...\n\nâœ… Expected: Human | Predicted: Human (77.5%)\nText: Just finished my morning coffee â˜• Ready to tackle the day!...\n\nâœ… Expected: Human | Predicted: Human (81.6%)\nText: Can't believe I forgot my keys again ğŸ¤¦â€â™‚ï¸ third time this week lol...\n\nâœ… Expected: Human | Predicted: Human (91.7%)\nText: Anyone else watching the game tonight? LFG!! ğŸˆ...\n\nâœ… Expected: Human | Predicted: Human (80.4%)\nText: Tried that new restaurant downtown - totally worth the hype! The pasta was *chef's kiss*...\n\nâŒ Expected: AI | Predicted: Human (77.9%)\nText: Statistical inference is the process of using data analysis to infer properties of an underlying dis...\n\nâŒ Expected: AI | Predicted: Human (54.4%)\nText: Machine learning algorithms utilize patterns in data to make predictions without being explicitly pr...\n\nâŒ Expected: AI | Predicted: Human (89.3%)\nText: The implementation of neural networks involves multiple layers of interconnected nodes that process ...\n\nâŒ Expected: AI | Predicted: Human (70.5%)\nText: Quantitative analysis demonstrates a significant correlation between the variables under investigati...\n\nâŒ Expected: AI | Predicted: Human (67.4%)\nText: The methodology employed in this study encompasses both qualitative and quantitative research approa...\n\nâœ… Expected: Human | Predicted: Human (87.8%)\nText: Honestly, I have no idea what I'm doing half the time but somehow it works out...\n\nâœ… Expected: Human | Predicted: Human (78.0%)\nText: My dog just stole my sandwich right off the table ğŸ˜‚ can't even be mad...\n\nâœ… Expected: Human | Predicted: Human (72.2%)\nText: Does anyone know a good place to buy cheap textbooks? College is expensive af...\n\nâŒ Expected: AI | Predicted: Human (54.6%)\nText: The algorithm processes input data through convolutional layers to extract relevant features for cla...\n\nâŒ Expected: AI | Predicted: Human (55.5%)\nText: Database normalization reduces redundancy and improves data integrity through systematic decompositi...\n\nâŒ Expected: AI | Predicted: Human (69.0%)\nText: Cloud computing infrastructure provides scalable resources through virtualization and distributed sy...\n\nâœ… Expected: Human | Predicted: Human (87.6%)\nText: The rain tapped gently on my window as I curled up with my favorite book and hot chocolate...\n\nâœ… Expected: Human | Predicted: Human (71.7%)\nText: Running late as usual, grabbed my bag and practically flew out the door...\n\nâœ… Expected: Human | Predicted: Human (61.9%)\nText: Sometimes I wonder if my plants judge me for forgetting to water them ğŸŒ±...\n\nâŒ Expected: AI | Predicted: Human (59.9%)\nText: Contemporary research indicates that climate change significantly impacts global biodiversity patter...\n\nâŒ Expected: AI | Predicted: Human (81.1%)\nText: Economic indicators suggest substantial fluctuations in market volatility during the observed period...\n\nâŒ Expected: AI | Predicted: Human (59.4%)\nText: The experimental results demonstrate statistically significant differences between the control and t...\n\n============================================================\nTRAINING COMPLETE!\n============================================================\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}